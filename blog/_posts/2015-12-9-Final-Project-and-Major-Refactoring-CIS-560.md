---
layout: blog
categories: blog 2015 12 9
excerpt: Final project for computer graphics - Approximate Agglomerative Clustering BVH, thread scheduling and concurrency, progressive rendering, sobol quasirandom sampling, multiple importance sampling
title: Final Project and Major Refactoring
---

<p>My final project for CIS 560 required some major code refactoring. My previous code was fairly inefficient and poorly structured. Without a well-built modular program it would have been significantly more difficult to get the project done. I've always been interested in getting my renders to complete faster, so I focused mostly on building optimizations into my pathtracer project. The first of these is BVH construction with Approximate Agglomerative Clustering.</p>

<h2>Multiple Importance Sampling</h2>
<p>This wasn't part of the final project, but it's new since I last posted. Multiple importance sampling is a vert important part of rendering. When calculating direct energy, we typicall cast rays from our point of intersection to a random point on the surface of a light source. However, if our light source is large but the surface is extremely specular, most of those rays are not going to bounce in the direction of our camera. It will take very many samples to even get a reasonably accurate image. We're wasting a lot of computation! On the other hand, if we only use the surface's reflectance model, if the light is small, a rough surface will generate rays that actually hit that light source a very small percentage of the time. We get the sample problem! The solution is to use Multiple Importance Sampling: use both methods and combine them with a <a href="https://graphics.stanford.edu/papers/combine/">heuristic</a> to calculate the direct energy reflected. Here's a recreation of the Veach scene and comparisons showing only BRDF or only Light sampling:</p>

<div class="row">
  <div class="col-sm-4 col-xs-12">
    <img src="/img/560/renders/brdf_only.bmp"/>
    <div class="caption">BRDF Sampling (Note: I think my sampling function is wrong. These surfaces appear far too specular</div>
  </div>
  <div class="col-sm-4 col-xs-12">
    <img src="/img/560/renders/veach_64spp_5min.bmp"/>
    <div class="caption">Multiple Importance Sampling</div>
  </div>
  <div class="col-sm-4 col-xs-12">
    <img src="/img/560/renders/light_only.bmp"/>
    <div class="caption">Light Sampling</div>
  </div>
</div>

<h2>Approximate Agglomerative Clustering</h2>
<p>Arguably one of the best ways to build a BVH is to use a bottom-up approach, merging the most optimal pair of nodes based on some heuristic function. The problem is, this is an extremely costly algorithm. If there are 100,000 nodes, it takes nCr(100,000, 2) evaluations of that heuristic function (once for every pair) to determine just one optimal pair. This is <i>O(n<sup>3</sup>)</i> to build the entire tree! <a href="http://www.cs.cmu.edu/~ygu1/paper/HPG13/HPG13.pdf">This paper</a> presents a novel idea, based on the idea that this operation is most expensive in the beginning of the construction when there are many, many nodes, and that many of those comparisons are very unecessary. We know that the optimal pair of nodes will be physically close to each other, so there is little point in checking every single pair of nodes. Instead, we can first organize them in a lesser quality tree and use that as a bound to limit the number of comparisons we make at every iteration. The nodes are continually separated until we reach some maximum node size. Then, only those nodes in that cluster are compared pairwise and merged. The process is sped up by first sorting the nodes by their <a href="https://en.wikipedia.org/wiki/Z-order_curve">Morton code</a> which quickly and easily forms the data into an linear tree-like form.</p>

<div class="row">
  <div class="col-sm-6 col-xs-12">
    <a href="/img/560/bvh/dragon.png"><img src="/img/560/bvh/dragon.png"/></a>
    <div class="caption">Stanford Dragon</div>
  </div>
  <div class="col-sm-6 col-xs-12">
    <a href="/img/560/bvh/bvh.png"><img src="/img/560/bvh/bvh.png"/></a>
    <div class="caption">BVH constructed in 1.5 sceconds</div>
  </div>
</div>

<p>I was pretty impressed by the results of this method. It allowed me to build a BVH for the Stanford dragon in just 1.5 seconds! This varies based on maximum cluster size. Longer build times of 12 seconds result in more efficient trees that are traversed more quickly. I haven't done any rigorous benchmarking on this yet, so I have yet to determine if the trees generated by this method are indeed well-structured. Regardless, I think that a lot of work needs to be done in otpimizing this construction. Especially in dense meshes, there's a very significant number of nodes which overlap. Testing intersections with the Stanford dragon can often take as long as 100ms! Even if they can be constructed very fast with this algorithm, it's probably not worth it if the resulting trees are not efficient.</p>


<h2>Massively Concurrent</h2>
<p>My pathtracer is now extremely concurrent. I made heavy use of <code>tbb::flow::graph</code>, <code>tbb::task_group</code>, and <code>tbb::task_arena</code> as well as TBB's <code>parallel_for</code> and <code>parallel_join</code> template functions to significantly improve the performance of my pathtracer. I created separate jobs for different parts of the render: creating ray jobs, tracing those rays through the scene, writing to the render film, updating the live GL preview window, etc... At first, I just created several <code>tbb::task_group</code>s for each of these, but this very quickly became confusing and difficult to manage. Creating temporary <code>tbb::concurrent_queue</code>s to transfer data between them, ensuring that none of the tasks prematurely finish or get deleted, properly instantiating them when necessary, and safely cleaning up and freeing memory became extremely difficult to handle. I'm still learning a lot about C++, and the problems exponentially increase when you have multiple threads running. Using <code>tbb::flow::graph</code> made everything significantly easier. I could very easily set up my tasks and define the data flow and TBB automagically handled communication and data transfer between all of the tasks. When I first started using this, I tried to be smart about my task allocation and tried to specify the maximum number of tasks TBB should allocate to each node in the graph. I soon learned, that it's very difficult to know exactly how much to allocate to each node. Depending on where they're going in a scene, some rays may trace quickly and some may trace slowly. Allowing TBB to handle task creation with <code>tbb::flow::unlimited</code> led to consistently superior performance.</p>

<p>I also employed TBB to accelerate my spatial acceleration tree traversal. I'm currently using a BVH and because nodes can very frequently overlap (so a piece of geometry could very easily be in multiple bounding boxes at once), I thought it would be a good idea to simultaneously explore the left and right subtrees. Intersected nodes were pushed onto a queue and then multiple threads pulled from that queue and checked intersections. I found that my assumption was indeed correct. On a simple scene (I have yet to do benchmarking on something more complex) one thread gave me intersections times of about 0.015ms on average. Two threads were nearly twice as fast at an everage of 0.064ms. Interestingly, using three threads was slightly slower than using two, but four threads was again faster. The reason for this is probably that because the tree is binary, a thread count which is a power of 2 is more unlikely to leave empty threads waiting around which may cause unecessary overhead.</p>

<div class="row">
  <div class="col-sm-6 col-xs-12">
    <img src="/img/560/renders/bunny_mirror_64spp_5min.bmp"/>
    <div class="caption"></div>
  </div>
  <div class="col-sm-6 col-xs-12">
    <img src="/img/560/renders/dragon_blinn_64spp_12min_BVHbuild_23sec.bmp"/>
    <div class="caption"></div>
  </div>
</div>

<h2>Progressive Rendering and Sobol Quasi-Random Sampling</h2>
<p>Not that much to say about this. One task creates random samples with a Sobol sequence and another recieves data from the <code>tbb::flow::graph</code> as each pixel is rendered and updates the GL preview window.</p>

<h2>Last Thoughts</h2>
<p>I was really hoping that I would have time to implement some other methods of integration like photon mapping or bidirectional pathtracing, but I didn't have nearly enough time. Perhaps if I have time in the future I will continue this project and develop those features. I'm not sure how soon that will happening because next year looks like it's going to be pretty busy with Physically Based Animation, Crowd Simulation, and TAing the introductory course to Computer Graphics!</p>

<div class="row">
  <div class="col-sm-6 col-xs-12">
    <img src="/img/560/renders/reflective_blinn_spheres_64spp_3min.bmp"/>
    <div class="caption">Specular/Blinn spheres</div>
  </div>
  <div class="col-sm-6 col-xs-12">
    <img src="/img/560/renders/refractive_spheres_64spp_3min.bmp"/>
    <div class="caption">Refractive spheres. There are even some visible caustics!</div>
  </div>
</div>